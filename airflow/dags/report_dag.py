import os
import json
import logging
from pathlib import Path
from datetime import timedelta

import pandas as pd
from airflow.models import DAG
from airflow.providers.postgres.hooks.postgres import PostgresHook
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from airflow.operators.python import PythonOperator
from airflow.utils.dates import days_ago


_LOG = logging.getLogger()
_LOG.addHandler(logging.StreamHandler())

BUCKET = "5e4f9aa0-52a3cd57-dc94-43c1-ae9a-f2724eeac656"
analysis_dirname = "/home/dk/easy_loans/airflow/analysis"

DEFAULT_ARGS = {
    "owner": "Denis",
    "email": "devsboom@gmail.com",
    "email_on_failure": True,
    "email_on_retry": False,
    "retry": 3,
    "retry_delay": timedelta(seconds=30)
}

dag = DAG(
    dag_id="report",
    schedule_interval="0 1 * * *",
    start_date=days_ago(2),
    catchup=False,
    tags=["mlops"],
    default_args=DEFAULT_ARGS
)


def get_transactions_from_db(**kwargs) -> str:
    _LOG.info("Report pipeline is initialized.")

    customer_id = kwargs.get("dag_run").conf.get("customer_id")
    upload_id = kwargs.get("dag_run").conf.get("upload_id")
    upload_start_date = kwargs.get("dag_run").conf.get("upload_start_date")
    upload_finish_date = kwargs.get("dag_run").conf.get("upload_finish_date")
    analysis_id = kwargs.get("dag_run").conf.get("analysis_id")

    pg_hook = PostgresHook("pg_connection")
    conn = pg_hook.get_conn()

    data = pd.read_sql_query(
        con=conn,
        sql=f"SELECT * FROM transactions WHERE customer_id = '{customer_id}' AND upload_id = '{upload_id}';"
    )
    data["date"] = pd.to_datetime(data["date"], infer_datetime_format=True).dt.strftime("%Y-%m-%d")

    s3_hook = S3Hook("s3_connector")
    session = s3_hook.get_session("ru-1")
    resource = session.resource("s3")
    json_byte_object = json.dumps(data.to_dict(), indent=4)
    resource.Object(BUCKET, f"transactions/{analysis_id}.json").put(Body=json_byte_object)

    _LOG.info("Successful upload of transactions json")

    return analysis_id


def save_transactions_json(**kwargs) -> None:
    dir_path = Path(analysis_dirname)
    if not os.path.exists(dir_path):
        dir_path.mkdir(parents=True)

    ti = kwargs.get("ti")
    name = ti.xcom_pull(task_ids="get_transactions_from_db")
    s3_hook = S3Hook("s3_connector")
    result_file = s3_hook.download_file(key=f"transactions/{name}.json",
                                        bucket_name=BUCKET,
                                        local_path=dir_path,
                                        preserve_file_name=True,
                                        use_autogenerated_subdir=False)

    _LOG.info(f"Successful download of transactions json: {result_file}!")


task_get_transactions_from_db = PythonOperator(task_id="get_transactions_from_db",
                                               python_callable=get_transactions_from_db,
                                               provide_context=True,
                                               dag=dag)

task_save_transactions_json = PythonOperator(task_id="save_transactions_json",
                                             python_callable=save_transactions_json,
                                             provide_context=True,
                                             dag=dag)


task_get_transactions_from_db >> task_save_transactions_json
